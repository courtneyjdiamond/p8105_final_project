---
title: "<span style='color: blue;'>Data Sources and Overview</span>"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: yeti
    code_folding: hide
css: styles.css
---

<style>
  h1 {
     font-weight: bold;
  }
</style>

  &nbsp;

```{r setup, include=FALSE}
library(tidyverse)
library(pdftools)
library(magrittr)
library(here)

knitr::opts_chunk$set(message = FALSE,
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

```
  
  ***
  
## <span style='color: blue;'>Data Sources</span>

[**CitiBike Data**](https://s3.amazonaws.com/tripdata/index.html): These datasets contain monthly logs of trip, including ride ID, the user's status (subscriber or casual user), trip start/end longitude/latitude coordinates, and the trip start/stop times and duration. 

[**Social Deprivation Index**](https://www.graham-center.org/maps-data-tools/social-deprivation-index.html): The social deprivation index (SDI) is an effort to generate a scoring system of socioeconomic factors using US census data from the American Community Survey. The final SDI is a composite measure of percent living in poverty, percent with less than 12 years of education, percent single-parent households, the percentage living in rented housing units, the percentage living in the overcrowded housing unit, percent of households without a car, and percentage nonemployed adults under 65 years of age. For more details please see the webpage link.

[**Overweight Data**](https://a816-dohbesp.nyc.gov/IndicatorPublic/Subtopic.aspx?theme_code=2,3&subtopic_id=113): This dataset contains publicly available data from NYC regarding the percent of people who are overweight per area. 

[**Air Quality Index Data**](https://data.cityofnewyork.us/Environment/Air-Quality/c3uy-2p5r): These data originated from NYC Open Data, contributed by the Department of Hygiene and Mental Health and include air quality indexes measured across boroughs in NYC.

  &nbsp;
  
  ***
  
## <span style='color: blue;'>Invididual Dataset Imports and Cleaning</span>

### Citibike

The citibike data was a collection of csv files that we downloaded and merged. We were specifically interested in 2019 as this was when SDI scores were available. We downloaded the data files from December 2018 through January 2020.

A look at the raw data: 

```{r import citibike}
citibike = 
  tibble(
    files = list.files("./citibike/"),
    path = str_c("./citibike/", files)
  ) |>
  mutate(data = map(path, ~read_csv(.x, col_types = cols(
    'end station id' = col_double(),
    'start station id' = col_double()
  )))) |>
  unnest(cols = c(data))

citibike |>
  head() |>
  knitr::kable()
```


The cleaning of the citibike data included the following steps:

 * Recoding gender from numeric to text
 * Filtering trip duration to more than 5 minutes but less than 1 day in order to capture true trips taken
 * Including trips that started in 2018 but ended in 2019 and trips that started in 2019 but ended in 2020, so as to include all trips taken in 2019
 * Creating an age at time of trip variable from birth year
 * Creating a trip duration in minutes (as opposed to seconds) variable
 * Renaming variables
 
 Note: throughout the code chunks we have removed large dataframes to save memory.
 
 A look at the tidied dataframe: 
 
```{r tidy citibike}
citibike_df = citibike |>
  janitor::clean_names() |>
  select(-files, -path) |>
  mutate(month = as.factor(month(starttime))) |>
  rename(trip_duration_sec = tripduration,
         start_time = starttime,
         stop_time = stoptime,
         user_type = usertype)|>
  mutate(gender = recode(gender,
                         "0" = "Unknown",
                         "1" = "Male",
                         "2" = "Female"),
        trip_duration_min = trip_duration_sec / 60,
        age = 2019 - birth_year
        ) |>
  filter(
    trip_duration_sec >= 300,
    trip_duration_sec <= 86400,
    as.Date(stop_time) >= as.Date("2019-01-01"),
    as.Date(start_time) <= as.Date("2019-12-31")
  )  |>
  select(trip_duration_sec, trip_duration_min, everything()) 

remove(citibike)

head(citibike_df) |>
  knitr::kable()

```




The citibike dataframe includes `r nrow(citibike_df)` trips and `r ncol(citibike_df)` variables. The variables in the dataframe are `r names(citibike_df)`. 

Due to large sample size that limits technical processing feasibility, we decided to focus on only a month of Citibike data. To do so, we look at the distribution of number of rides by month. 

Before limiting the data to September, we want to get a list of all stations in the dataset for future exploration:
```{r unique_stations}
start_station = citibike_df |>
  distinct(start_station_id, start_station_name,
         start_station_latitude, start_station_longitude) |>
  rename(
    station_id = start_station_id,
    station_name = start_station_name,
    latitude = start_station_latitude,
    longitude = start_station_longitude
  )

end_station = citibike_df |>
  distinct(end_station_id, end_station_name,
         end_station_latitude, end_station_longitude) |>
  rename(
    station_id = end_station_id,
    station_name = end_station_name,
    latitude = end_station_latitude,
    longitude = end_station_longitude
  )


stations = rbind(start_station, end_station) |>
  unique() 
  

```


```{r citibike_plot}
citibike_df |>
  group_by(month) |>
  summarize(n_rides = n()) |>
  arrange(month) |>
  ggplot(aes(x = month, y = n_rides, group = 1)) +
  geom_point() +
  geom_line() 
```

#### Filter to September
Since September is the month of highest usage, we focus on September
```{r filter_month}
citibike_df = citibike_df |> filter(month == 9)
```

The September Citibike dataframe includes `r nrow(citibike_df)` trips.

  &nbsp;
  
### Social Deprivation Index

The SDI dataset was a csv. We renamed the zipcode variable to be more intuitive. As we were only interested in the entirety of the SDI score, we filtered the data to include zip code and sdi_score only. 

* Note: the higher the SDI score, the higher the social deprivation in that area.

A look at the tidied dataframe: 

```{r sdi_import}
SDI_df <- read_csv("./data/SDI_data/rgcsdi-2015-2019-zcta.csv") |>
  janitor::clean_names() |>
  select(zcta5_fips, sdi_score)|>
  rename(zip=zcta5_fips)

head(SDI_df) |>
  knitr::kable()
```

  &nbsp;
  
### Overweight data 

The overweight data was downloaded as csv files for "Overweight and Obese Adults." We were specifically interested in 2019 as this was when SDI scores were available.

The cleaning of the overweight data included the following steps:

* Converting percent to a numeric variable
* Separating out the high and low confidence intervals of percent
* Renaming time to year
* Filtering data to 2019

A look at the tidied dataframe:

```{r overweight_import}
overweight_df =
  read_csv('./data/SDI_data/nyc_overweight_or_obesity_adults.csv') |>
  janitor::clean_names() |>
  mutate(number = gsub("\\*", "", number)) |>
  mutate(number = gsub(",", "", number)) |>
  mutate(number = as.numeric(number)) |>
  mutate(
    percent_low = as.numeric(gsub("^.*\\(\\s*", "", gsub("\\s*,.*$", "", percent))),
    percent_high = as.numeric(gsub("^.*\\,\\s*", "", gsub("\\)$", "", percent))),
percent = as.numeric(ifelse(grepl("\\*", percent),
                                gsub("\\*.*$", "", percent),
                                gsub("\\s*\\(.*", "", percent)))) |>
  rename(year = time) |>
  filter (year == 2019)

head(overweight_df) |>
  knitr::kable()
#write_csv(overweight_df, "data/SDI_data/overweight_data_clean.csv")

```

  &nbsp;
  
### Air Quality

The air quality data was a downloaded from csv files. 

The cleaning of the air quality index data included the following steps:

* Extracting year from the date variable 
* Filtering data to 2019
* Filtering to geolocation type UHF34, as this is consistent with our other datasets (more on this below!)
* Filtering to annual values, as this dataset also has seasonal values
* Filtering to Fine Particles (PM 2.5) (the original dataset also had Nitrogen dioxide (NO2) and Ozone (O3), but for the purpose of only a single value for AQ, we keep PM 2.5 as the most relevant 

A look at the tidied dataset: 

```{r air_quality}
air_quality_df = read_csv("./data/air_quality/Air_Quality_20231126.csv") |>
  janitor::clean_names() |>
  mutate(
    start_date = mdy(start_date),
    year = year(start_date)
  ) |>
  filter(year == "2019")

#Filter to only annual averages of fine particles (PM 2.5) mean measurement, mcg/m3
air_quality_df =   
  air_quality_df |>
  filter(geo_type_name == "UHF34") |>
  filter(time_period == "Annual Average 2019") |>
  filter(name == "Fine particles (PM 2.5)")

head(air_quality_df) |>
  knitr::kable()
```

The air quality dataframe now includes `r nrow( air_quality_df)` rows and `r ncol( air_quality_df)` variables. The variables in the dataframe are `r names( air_quality_df)`.

  ***
  
## <span style='color: blue;'>NYC Neighborhood Data Merging</span>

To combine datasets, they all need to have the same geographic identifier. We decided to use United Hospital (UHF) 34 codes which consists of 34 adjoining zip codes with similar characteristics compiled by NYC Gov. We used UHF 34 (instead of other UHF 42) as UHF 34 had the least number of neighborhoods and is therefore more manageable (contains the largest area per neighborhood).

  * For more information about the background behind UHF codes, please see [**here**](https://a816-dohbesp.nyc.gov/IndicatorPublic/data-stories/geographies/). 

We did the following:

1. **UHF42 to Zipcode** First we converted United Hospital (UHF) 42 codes to zip codes, as the overweight dataframe included UHF 42 codes. We found a table that mapped UHF 42 codes to zip codes from a NYC gov  [**pdf**](https://www.nyc.gov/assets/doh/downloads/pdf/ah/zipcodetable.pdf`). Below we import this table and tidied it into a dataframe


```{r uhf_zip}
uhf_zip = 
  pdf_text("./data/geocoding/zipcodetable.pdf") |> 
  extract(1)

uhf_zip_df =
  uhf_zip |> 
  str_split("\n") |>
  tibble() |> 
  rename("data" = "str_split(uhf_zip, \"\\n\")") |> 
  unnest("data") |> 
  filter(data != "") |> 
  filter(!row_number() %in% c(1, 2, 51)) |> 
  filter(!row_number() %in% c(1, 2, 10, 22, 33, 44)) |> 
  mutate(data = str_squish(data)) |> 
  mutate(data = str_remove_all(data, "-")) |> 
  mutate(data = str_squish(data)) |> 
  separate(data, into = c("uhf", "rest"), sep = "(?<=\\d\\s)") |> 
  separate(col = "rest", into = c("neighborhood", "zip"), sep = "(?=\\s\\d)", extra = "merge") |> 
  separate(col = "zip", into = c("zip1", "zip2", "zip3", "zip4", "zip5", "zip6", "zip7", "zip8", "zip9"), sep = ",") |> 
  mutate(across(everything(), ~ str_trim(.x))) |> 
  mutate(across(c("zip1":"zip9"), ~ as.numeric(.x))) |> 
  mutate(uhf = as.numeric(uhf)) |> 
  pivot_longer(c(zip1:zip9), names_to = "zip_name", values_to = "zip") |> 
  filter(!is.na(zip)) |> 
  select(!zip_name)
```


2. **UHF34 to UHF42**  We imported and cleaned a dataframe that converted UHF 34 to UHF 42 via neighborhood name

```{r uhf}
uhf_34 = 
  pdf_text("./data/geocoding/uhf34.pdf")

uhf_34_df =
  uhf_34 |> 
  str_split("\n") |>
  tibble() |> 
  rename("data" = "str_split(uhf_34, \"\\n\")") |> 
  unnest("data") |> 
  filter(data != "") |> 
  mutate(data = str_squish(data)) |> 
  filter(!row_number() %in% c(1, 2, 7, 14, 18, 21, 24, 27, 28, 31, 32, 34, 35, 39, 40, 42, 43, 44, 45, 47, 48, 52, 53, 57, 61:75)) |> 
  mutate(data = str_replace(data, "Kingsbridge - Riverdale", "101 Kingsbridge - Riverdale")) |> 
  mutate(data = str_replace(data, "Northeast Bronx", "102 Northeast Bronx")) |> 
  filter(!row_number() %in% c(1,2)) |> 
  mutate(data = str_remove_all(data, "-"),
         data = str_squish(data), 
         data = str_trim(data)) |>
  mutate(data = str_remove(data, "(\\d)+$")) |>
  mutate(data = str_remove(data, "(\\d)+\\s$")) |>
  separate(data, into = c("uhf34", "neighborhood"), sep = "(?<=\\d\\s)") |> 
  mutate(uhf34 = as.numeric(str_trim(uhf34)),
         neighborhood = str_trim(neighborhood)) |> 
  mutate(uhf2 = uhf34) |> 
  separate_wider_position(uhf2, widths = c("1_uhf" = 3, "2_uhf" = 3, "3_uhf" = 3), too_few = "align_start") |> 
  pivot_longer(c("1_uhf":"3_uhf"), names_to = "uhf_name", values_to = "uhf42") |> 
  select(!uhf_name) |> 
  mutate(uhf34 = as.numeric(uhf34),
         uhf42 = as.numeric(uhf42))|>
  mutate(borough = case_when(
    substring(uhf34, 1, 1) == "1" ~ "Bronx",
    substring(uhf34, 1, 1) == "2" ~ "Brooklyn",
    substring(uhf34, 1, 1) == "3" ~ "Manhattan",
    substring(uhf34, 1, 1) == "4" ~ "Queens",
    substring(uhf34, 1, 1) == "5" ~ "Staten Island",
    TRUE ~ "Unknown"  # Handle any other values
  ))
  
```

3. **UHF34, UHF42, Zip** We combined UHF 42 and UHF 34 dataframes and zipcode to create a neighborhood crosswalk.

```{r uhf34_42}
joined_uhf_34_42 = 
  uhf_zip_df |> 
  left_join(y = uhf_34_df, by = join_by("uhf" == "uhf42")) |> 
  rename("uhf34_neighborhood" = "neighborhood.y", 
         "uhf42_neighborhood" = "neighborhood.x", 
         "uhf42" = "uhf")

```

  ***
  
## <span style='color: blue;'>Bring Neighborhood to Health Datasets</span>


Now that we have a crosswalk among all location identifiers (UHF34, UHF42, and zipcode) used in our datasets, we'll make sure they all have UHF34 so they can be merged together later. 

### Join SDI data to UHF/Zip/Neighborhood data
```{r SDI_zip}
joined_SDI_zip_neighborhood = 
  SDI_df |> 
  filter(zip %in% pull(joined_uhf_34_42, zip)) |> 
  mutate(zip = as.numeric(zip)) |> 
  left_join(y = joined_uhf_34_42, by = "zip")
```

### Join Overweight data to UHF/Zip/Neighborhood data
```{r overweight_zip}
joined_overweight_zip_neighborhood = 
  overweight_df |> 
  left_join(y = joined_uhf_34_42, by = join_by("geo_id" == "uhf34")) |>
  rename(percent_overweight = percent)

```

### Join Air Quality data to UHF/Zip/Neighborhood data
```{r air_quality_uhf}
#Align UHF neighborhood names
air_quality_df = 
  left_join(air_quality_df, uhf_34_df, by = c("geo_join_id" = "uhf34"))

air_quality_df =
  air_quality_df |>
select(data_value, neighborhood ) |>
 distinct()
```

  ***
  
## <span style='color: blue;'>Bring Neighborhood to Citibike Dataset</span>

The Citibike data is coded by latitude/longitude To enable merging of Citibike data with the health datasets, we need to transform coordinates to zipcode, that can later be easily converted to UHF34 identifiers.

* To get zipcodes for each  latitude/longitude pair in the Citibike data, we used the [tidygeocoder](https://jessecambon.github.io/tidygeocoder/) packager. The full code with details of how that was created [can be found here](data/geocoding/latlong_to_zip.html). Here, we import the latitude/longitude to zipcode crosswalk that we generated. 

  * Note: Every Citibike entry has a start_latitude/longitude, and end_latitude/longitude. Each row will accordingly have a start_zipcode and end_zipcode. 
  
### Merge Citibike with Zipcode

```{r citibike_zip}
latlong_zip = read_csv('./data/geocoding/citibike_latlong_zip.csv')

citibike_zip =
  citibike_df |>
  left_join(latlong_zip, 
            by = c("start_station_latitude" = "latitude",
                   "start_station_longitude" = "longitude")) |>
  rename("start_zipcode" = "postcode") |>
  left_join(latlong_zip, 
            by = c("end_station_latitude" = "latitude",
                   "end_station_longitude" = "longitude")) |>
  rename("end_zipcode" = "postcode") 

remove(citibike_df)

```

There are `r citibike_zip |> filter(is.na(start_zipcode) | is.na(end_zipcode)) |> nrow()` entries missing either the start or end zipcode.


#### *Missing Zipcodes*

A look at the start coordinates with missing zipcodes below: 

```{r missing_zip_start}
citibike_zip |>
  filter(is.na(start_zipcode)) |>
  group_by(start_station_name, start_station_latitude, start_station_longitude) |>
  summarize(n = n()) |> 
  knitr::kable()
```

A look at the end coordinates with missing zipcodes below: 

```{r missing_zip_end}
citibike_zip |>
  filter(is.na(end_zipcode)) |>
  group_by(end_station_name, end_station_latitude, end_station_longitude) |>
  summarize(n = n()) |>
  knitr::kable()

```


#### *Manual Zipcode Completion*
For the above missing zipcodes, we manually looked up based on latitude, longitude, and station_name what the correct zipcode is. Upon manually exploration, these looked like some data quality issues with the coordinates, but based on the staton name we were able to identify the correct zipcode. We stored this work in a csv on our g[ithub here](https://github.com/courtneyjdiamond/pedalstopatterns.github.io/blob/main/data/geocoding/manual_zipcodes.csv). The code below implements filling in those missing zipcode values. 


```{r manual_zip}
citibike_zip = citibike_zip |>
  mutate(start_zipcode = case_when(start_station_name == "Broadway & W 32 St" ~ 10001,
                               start_station_name == "Cooper Square & Astor Pl" ~ 10003,
                               start_station_name == "William St & Pine St" ~ 10005,
                               start_station_name %in% c("Broadway & W 36 St","Broadway & W 37 St","Broadway & W 38 St","Broadway & W 41 St") ~ 10018,
                               start_station_name == "W 52 St & 6 Ave" ~ 100019,
                               start_station_name %in% c("Broadway & W 41 St","W 43 St & 6 Ave") ~ 10036,
                               start_station_name == "South St & Gouverneur Ln" ~ 10043,
                               start_station_name == "E 47 St & 1 Ave" ~ 10075,
                               start_station_name == "Roebling St & N 4 St" ~ 11211,
                               TRUE ~ start_zipcode)) |>
  mutate(end_zipcode = case_when(end_station_name == "Broadway & W 32 St" ~ 10001,
                               end_station_name == "Cooper Square & Astor Pl" ~ 10003,
                               end_station_name == "William St & Pine St" ~ 10005,
                               end_station_name %in% c("Broadway & W 36 St","Broadway & W 37 St","Broadway & W 38 St","Broadway & W 41 St") ~ 10018,
                               end_station_name == "W 52 St & 6 Ave" ~ 100019,
                               end_station_name %in% c("Broadway & W 41 St","W 43 St & 6 Ave") ~ 10036,
                               end_station_name == "South St & Gouverneur Ln" ~ 10043,
                               end_station_name == "E 47 St & 1 Ave" ~ 10075,
                               end_station_name == "Roebling St & N 4 St" ~ 11211,
                               TRUE ~ end_zipcode))

```

###  Merge Citibike with UHF34 Neighborhood
Now that we have the zipcode for Citibike data, we can use the zipcode and UHF34 crosswalk generated above to get neighborhood onto Citibike data. 

```{r citibike_neighborhoods}
citibike_zip_neighborhoods =
  citibike_zip |>
  left_join(y = (joined_uhf_34_42 |> select(zip, uhf42_neighborhood, uhf34_neighborhood)),
            by = join_by("start_zipcode" == "zip")) |> 
  rename("start_uhf34_neighborhood" = "uhf34_neighborhood",
         "start_uhf42_neighborhood" = "uhf42_neighborhood") |> 
  left_join(y = (joined_uhf_34_42 |> select(zip, uhf42_neighborhood, uhf34_neighborhood)),
            by = join_by("end_zipcode" == "zip")) |> 
  rename("end_uhf34_neighborhood" = "uhf34_neighborhood",
         "end_uhf42_neighborhood" = "uhf42_neighborhood")

remove(citibike_zip)

head(citibike_zip_neighborhoods) |> 
  knitr::kable()
```

#### *Missing UHF34*

There were some citibike zipcodes with missing UHF34, which we explore below: 

```{r missing_uhf}
missing_startuhf = citibike_zip_neighborhoods |>
  filter(is.na(start_uhf34_neighborhood)) |>
  group_by(start_zipcode) |>
  summarize(n = n()) |>
  rename(zipcode = start_zipcode)
  
missing_enduhf = citibike_zip_neighborhoods |>
  filter(is.na(end_uhf34_neighborhood)) |>
  group_by(end_zipcode) |>
  summarize(n = n()) |>
  rename(zipcode = end_zipcode)

#write_csv(rbind(missing_startuhf, missing_enduhf) ,'../data/geocoding/missing_uhf34.csv')

citibike_zip_neighborhoods = citibike_zip_neighborhoods |>
  filter(!is.na(end_uhf34_neighborhood) & !is.na(start_uhf34_neighborhood))


```

There are `r missing_startuhf |> pull(n) |> sum()` entries whose start zipcode do not have an associated uhf34, and  `r missing_enduhf |> pull(n) |> sum()` entries whose end zipcode do not have an associated uhf34. Manual validation showed these zipcodes are business zipcodes, where a zipcode refers to a small domain of a business. While these could be pinpointed roughly to a UHF34 neighborhood manually, it is cleaner to simply omit them, as we still retain a substantial dataset size. The resulting dataset has `r citibike_zip_neighborhoods |> nrow()` entries. 

  ***
  
## <span style='color: blue;'>Final Dataset: Merge Citibike, SDI, Overweight, and AQ</span>

Next we needed to create one large dataframe that merged the citibike data with and each health dataset by UHF 34 neighborhood.

### Merge SDI and Overweight data 
SDI and overweight are joined together first by UHF42. 

* Note: There are rows in the overweight data without zipcode that refer instead to borough and city-wide overweight percentages. These are excluded from joining on SDI/citibike data, which function by zipcode that is more granular than borough. These are what is excluded by filtering out null values. 

```{r join_SDI_overweight}
sdi_overweight = 
  merge(joined_overweight_zip_neighborhood, joined_SDI_zip_neighborhood,
                   by.x = c("zip", "uhf42", "uhf42_neighborhood"),
                   by.y = c("zip", "uhf42", "uhf42_neighborhood"),
                   all.x = TRUE)  |>
  select("zip","uhf34","uhf34_neighborhood.x","uhf42","uhf42_neighborhood",
         "sdi_score","percent_overweight", "borough.x") |>
  rename(uhf34_neighborhood = uhf34_neighborhood.x,
         borough = borough.x) |>
  filter(!is.na(zip)) 

remove(joined_overweight_zip_neighborhood)
remove(joined_SDI_zip_neighborhood)
remove(SDI_df)
remove(overweight_df)
```

### Merge SDI and Overweight data onto citibike
```{r SDI_Overweight_join}
citibike_df =
  citibike_zip_neighborhoods |>
  left_join(y = sdi_overweight,
            by = join_by("start_zipcode" == "zip")) |>
  rename("start_sdi_score" = "sdi_score",
         "start_percent_overweight" = "percent_overweight",
         "start_borough" = "borough") 

remove(citibike_zip_neighborhoods)

citibike_df = citibike_df |>
  left_join(y = sdi_overweight,
            by = join_by("end_zipcode" == "zip")) |>
  rename("end_sdi_score" = "sdi_score",
         "end_percent_overweight" = "percent_overweight",
         "end_borough" = "borough") 
```

### Merge AQ data onto citibike

```{r AQ_join} 
#Join to final dataframe
citibike_df =
  citibike_df |>
  left_join(y = air_quality_df,
            by = join_by("start_uhf34_neighborhood" == "neighborhood")) |>
  rename("start_aq" = "data_value")

citibike_df = citibike_df |>
  left_join(y = air_quality_df,
            by = join_by("end_uhf34_neighborhood" == "neighborhood")) |>
  rename("end_aq" = "data_value")

```

### Final Tidy of Citibike Data 

A view at the final dataset, as well as overall summary of variables included and their missingness:

```{r final_citibike_tidy}
citibike_df = citibike_df |>
  select(bikeid, user_type, gender, age,
         start_time, stop_time, 
         start_station_latitude, start_station_longitude, 
         end_station_latitude, end_station_latitude,
         start_station_id, start_station_name,
         start_zipcode, start_uhf34_neighborhood,
         end_station_id, end_station_name,
         end_zipcode, end_uhf34_neighborhood,
         start_sdi_score, start_percent_overweight, start_aq,
         end_sdi_score, end_percent_overweight, end_aq, end_borough, start_borough)

head(citibike_df) |>
  knitr::kable()

skimr::skim(citibike_df)
```

```{r}
write_csv(citibike_df, file = './citibike_clean/citibike_clean.csv')
```

The tidied citibike dataframe includes `r nrow(citibike_df)` trips and `r ncol(citibike_df)` variables named `r names(citibike_df)`.


# Stations Analysis 
For some extra analysis work, we want to understand the full availability of stations, not just the ones used in September, So we also have to join zipcode and UHF34 neighborhood to our full station data.
```{r station_uhf}
stations = stations |>
  left_join(latlong_zip, 
            by = c("latitude" ,"longitude")) |>
  rename("zipcode" = "postcode") |>
  mutate(zipcode = case_when(station_name == "Broadway & W 32 St" ~ 10001,
                             station_name == "Cooper Square & Astor Pl" ~ 10003,
                             station_name == "William St & Pine St" ~ 10005,
                             station_name %in% c("Broadway & W 36 St",
                                                 "Broadway & W 37 St",
                                                 "Broadway & W 38 St",
                                                 "Broadway & W 41 St") ~ 10018,
                             station_name == "W 52 St & 6 Ave" ~ 100019,
                             station_name %in% c("Broadway & W 41 St",
                                                 "W 43 St & 6 Ave") ~ 10036,
                             station_name == "South St & Gouverneur Ln" ~ 10043,
                             station_name == "E 47 St & 1 Ave" ~ 10075,
                             station_name == "Roebling St & N 4 St" ~ 11211,
                             TRUE ~ zipcode)) |>
  left_join(y = (joined_uhf_34_42 |> select(zip, uhf34, uhf34_neighborhood, borough)),
            by = join_by("zipcode" == "zip")) 

write_csv(stations, "./citibike_clean/stations.csv")

```

