---
title: "<span style='color: blue;'>Data Sources and Overview</span>"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: yeti
css: styles.css
---

<style>
  h1 {
     font-weight: bold;
  }
</style>

  &nbsp;
  
```{r library, include = FALSE}
library(tidyverse)
library(shiny)
library(shinyjs)
shinyjs::useShinyjs()
```  
  
  ***
  
## <span style='color: blue;'>Data Sources</span>

 &nbsp;

[**CitiBike Data**](https://s3.amazonaws.com/tripdata/index.html): These datasets contain monthly logs of trip, including ride ID, the user's status (subscriber or casual user), trip start/end longitude/latitude coordinates, and the trip start/stop times and duration. 

[**Social Deprivation Index**](https://www.graham-center.org/maps-data-tools/social-deprivation-index.html): The social deprivation index (SDI) is an effort to generate a scoring system of socioeconomic factors using US census data from the American Community Survey. The final SDI is a composite measure of percent living in poverty, percent with less than 12 years of education, percent single-parent households, the percentage living in rented housing units, the percentage living in the overcrowded housing unit, percent of households without a car, and percentage nonemployed adults under 65 years of age. For more details please see the webpage link.

[**Overweight Data**](https://a816-dohbesp.nyc.gov/IndicatorPublic/Subtopic.aspx?theme_code=2,3&subtopic_id=113): This dataset contains publicly available data from NYC regarding the percent of people who are overweight per area.  

[**Air Quality Index Data**](https://data.cityofnewyork.us/Environment/Air-Quality/c3uy-2p5r): These data originated from NYC Open Data, contributed by the Department of Hygiene and Mental Health and include air quality indexes measured across boroughs in NYC.

[**Weather Data**](https://www.ncei.noaa.gov/metadata/geoportal/rest/metadata/item/gov.noaa.ncdc:C00861/html): This data is from the Global Historical Climatology Network - Daily (GHCN-Daily) dataset published by the National Centers for Environmental Information (NCEI) and include the weather station identifier, the date, latitude/longitude, maximum temperature, minimum temperature, and precipitation. 

  &nbsp;

***

## <span style='color: blue;'>Individual Data Imports and Cleaning</span>

  &nbsp;

### *Citibike* ###

   &nbsp;

##### Data Import #####

The citibike data was a collection of csv files that we downloaded and merged. We were specifically interested in 2019 as this was when SDI scores were available.


```{r import citibike, eval= FALSE}
citibike = 
  tibble(
    files = list.files("citibike"),
    path = str_c("citibike/", files)
  ) |>
  mutate(data = map(path, ~read_csv(.x, col_types = cols(
    'end station id' = col_double(),
    'start station id' = col_double()
  )))) |>
  unnest(cols = c(data))
```

```{r show first few rows of uncleaned citibike data, eval = FALSE}
head(citibike) |> 
  knitr::kable
```

(Will not include actual code for now as I cannot figure out how to knit)

The citibike dataframe includes r nrow(citibike) trips and r ncol(citibike) variables. The variables in the dataframe are r names(citibike). 

  &nbsp;

##### Data Cleaning #####

The cleaning of the citibike data included the following steps:

 * Recoding gender from numeric to text
 * Filtering trip duration to more than 5 minutes but less than 1 day in order to capture true trips taken
 * Including trips that started in 2018 but ended in 2019 and trips that started in 2019 but ended in 2020, so as to include all trips taken in 2019
 * Creating an age at time of trip variable from birth year
 * Creating a trip duration in minutes (as opposed to seconds) variable
 
```{r tidy citibike, eval = FALSE}
citibike_df <- citibike |>
  janitor::clean_names() |>
  select(-files, -path) |>
  rename(trip_duration_sec = tripduration,
         start_time = starttime,
         stop_time = stoptime,
         user_type = usertype)|>
  mutate(gender = recode(gender,
                         "0" = "Unknown",
                         "1" = "Male",
                         "2" = "Female"),
        trip_duration_min = trip_duration_sec / 60,
        age = 2019 - birth_year
        ) |>
  filter(
    trip_duration_sec >= 300,
    trip_duration_sec <= 86400,
    as.Date(stop_time) >= as.Date("2019-01-01"),
    as.Date(start_time) <= as.Date("2019-12-31")
  )  |>
  select(trip_duration_sec, trip_duration_min, everything())

```

The table below displays the citibike tidied data.

```{r show first few rows of cleaned citibike data, eval = FALSE}
citibike_df |>
  head() |>
  knitr::kable()
```

The citibike dataframe includes r nrow(citibike_df) trips and r ncol(citibike_df) variables. The variables in the dataframe are r names(citibike_df). 

  &nbsp;

### *Social Deprivation Index* ###

   &nbsp;

##### Data Import and Cleaning #####

The SDI dataset was a csv. We renamed the zip code variable to be more intuitive. As we were only interested in the entirety of the SDI score, we filtered the data to include zip code and sdi_score only. 

* It is important to note that the higher the SDI score the higher the social deprivation in that area.

```{r sdi import and clean, eval = FALSE}
SDI_df <- read_csv("data/SDI_data/rgcsdi-2015-2019-zcta.csv") |>
  janitor::clean_names() |>
  select(zcta5_fips, sdi_score)|>
  rename(zip=zcta5_fips)
```

This table shows the zip codes with the least social deprivation (ie the lowest SDI score).

```{r sdi table, eval = FALSE}
SDI_df |>
  arrange(sdi_score) |>  
  head(5) |>
  knitr::kable()
```

 &nbsp;

### *Overweight Data* ###

   &nbsp;

##### Data Import #####

The overweight data was a  csv files that we downloaded and merged. We were specifically interested in 2019 as this was when SDI scores were available.

```{r import overweight data, eval = FALSE}
overweight = read_csv('./data/SDI_data/nyc_overweight_or_obesity_adults.csv') |>
    janitor::clean_names()
```

The overweight dataframe includes r nrow(overweight) trips and r ncol(overweight) variables. The variables in the dataframe are r names(overweight). 

 &nbsp;

##### Data Cleaning #####

The cleaning of the overweight data included the following steps:

* Converting percent from a character variable to a numeric variable
* Separating out the high and low confidence intervals of percent
* Renaming time to year
* Filtering data to 2019

```{r tidy overweight , eval = FALSE}
overweight_df =
 overweight |>
  mutate(number = gsub("\\*", "", number)) |>
  mutate(number = gsub(",", "", number)) |>
  mutate(number = as.numeric(number)) |>
  mutate(
    percent_low = as.numeric(gsub("^.*\\(\\s*", "", gsub("\\s*,.*$", "", percent))),
    percent_high = as.numeric(gsub("^.*\\,\\s*", "", gsub("\\)$", "", percent))),
percent = as.numeric(ifelse(grepl("\\*", percent),
                                gsub("\\*.*$", "", percent),
                                gsub("\\s*\\(.*", "", percent)))) |>
  rename(year = time)|>
  filter (year == 2019)
```

The table below displays the overweight tidied data.

```{r show first few rows of cleaned overweight data, eval = FALSE}
overweight_df |>
  head() |>
  knitr::kable()
```

The overweight dataframe now includes r nrow(overweight_df) trips and r ncol(overweight_df) variables. The variables in the dataframe are r names(overweight_df). 

### *Air Quality Index* ###

   &nbsp;

##### Data Import #####

The air quality data was a  csv files. 

```{r air quality, eval = FALSE}
air <- read_csv("data/air_quality/Air_Quality_20231126.csv") 
```

The air dataframe includes r nrow(air) trips and r ncol(air) variables. The variables in the dataframe are r names(air). 

 &nbsp;

##### Data Cleaning #####

The cleaning of the air quality index data included the following steps:

* Separating out year 
* Filtering data to 2019

```{r air quality tidy, eval = FALSE}
 air_quality_df =air |>
  janitor::clean_names() |>
  mutate(
    start_date = mdy(start_date),
    year = year(start_date)
  ) |>
  filter(year == "2019")
```

The table below displays the air quality tidied data.

```{r show first few rows of cleaned air quality data, eval = FALSE}
 air_quality_df |>
  head() |>
  knitr::kable()
```

The air quality dataframe now includes r nrow( air_quality_df) trips and r ncol( air_quality_df) variables. The variables in the dataframe are r names( air_quality_df).

***

## <span style='color: blue;'>Data Merging by NYC Neighborhood</span>

Our end goal was to merge data by neighborhood. We decided to use United Hospital (UHF) 42 codes which consist of 34 adjoining zip codes with similar characteristics compiled by NYC Gov. We decideded to use UFH 34 instead of other versions as UFH 34 had the least number of neighborhoods (contains the largest area per neighborhood)

  * For more information about the background behind UHF codes, please see [**here**](https://a816-dohbesp.nyc.gov/IndicatorPublic/data-stories/geographies/). 


To do this, we did the following:

1. First we combined United Hospital (UHF) 42 codes to zip codes, as the overweight dataframe included UHF 42 codes. We found a table that mapped UFH 42 codes to zip codes from a NYC gov  [**pdf**](https://www.nyc.gov/assets/doh/downloads/pdf/ah/zipcodetable.pdf`). We imported this table and tidied it into a dataframe.

```{r import and tidy UFH42 pdf, eval = FALSE}
# install.packages("pdftables")
# install.packages("pdftools")

uhf_zip = 
  pdf_text("data/geocoding/zipcodetable.pdf") |> 
  extract(1)

uhf_zip_df =
  uhf_zip |> 
  str_split("\n") |>
  tibble() |> 
  rename("data" = "str_split(uhf_zip, \"\\n\")") |> 
  unnest("data") |> 
  filter(data != "") |> 
  filter(!row_number() %in% c(1, 2, 51)) |> 
  filter(!row_number() %in% c(1, 2, 10, 22, 33, 44)) |> 
  mutate(data = str_squish(data)) |> 
  mutate(data = str_remove_all(data, "-")) |> 
  mutate(data = str_squish(data)) |> 
  separate(data, into = c("uhf", "rest"), sep = "(?<=\\d\\s)") |> 
  separate(col = "rest", into = c("neighborhood", "zip"), sep = "(?=\\s\\d)", extra = "merge") |> 
  separate(col = "zip", into = c("zip1", "zip2", "zip3", "zip4", "zip5", "zip6", "zip7", "zip8", "zip9"), sep = ",") |> 
  mutate(across(everything(), ~ str_trim(.x))) |> 
  mutate(across(c("zip1":"zip9"), ~ as.numeric(.x))) |> 
  mutate(uhf = as.numeric(uhf)) |> 
  pivot_longer(c(zip1:zip9), names_to = "zip_name", values_to = "zip") |> 
  filter(!is.na(zip)) |> 
  select(!zip_name)
```

2. We imported and cleaned a dataframe that converted UFH 34 to zip code.

```{r import and tidy UFH 34 pdf, eval = FALSE}
uhf_34 = 
  pdf_text("data/geocoding/uhf34.pdf")

uhf_34_df =
  uhf_34 |> 
  str_split("\n") |>
  tibble() |> 
  rename("data" = "str_split(uhf_34, \"\\n\")") |> 
  unnest("data") |> 
  filter(data != "") |> 
  mutate(data = str_squish(data)) |> 
  filter(!row_number() %in% c(1, 2, 7, 14, 18, 21, 24, 27, 28, 31, 32, 34, 35, 39, 40, 42, 43, 44, 45, 47, 48, 52, 53, 57, 61:75)) |> 
  mutate(data = str_replace(data, "Kingsbridge - Riverdale", "101 Kingsbridge - Riverdale")) |> 
  mutate(data = str_replace(data, "Northeast Bronx", "102 Northeast Bronx")) |> 
  filter(!row_number() %in% c(1,2)) |> 
  mutate(data = str_remove_all(data, "-"),
         data = str_squish(data), 
         data = str_trim(data)) |>
  mutate(data = str_remove(data, "(\\d)+$")) |>
  mutate(data = str_remove(data, "(\\d)+\\s$")) |>
  separate(data, into = c("uhf34", "neighborhood"), sep = "(?<=\\d\\s)") |> 
  mutate(uhf34 = as.numeric(str_trim(uhf34)),
         neighborhood = str_trim(neighborhood)) |> 
  mutate(uhf2 = uhf34) |> 
  separate_wider_position(uhf2, widths = c("1_uhf" = 3, "2_uhf" = 3, "3_uhf" = 3), too_few = "align_start") |> 
  pivot_longer(c("1_uhf":"3_uhf"), names_to = "uhf_name", values_to = "uhf42") |> 
  select(!uhf_name) |> 
  mutate(uhf34 = as.numeric(uhf34),
         uhf42 = as.numeric(uhf42))
  
```

3. Lastly we combined UFH 42 and UFH 34 dataframes.

```{r, eval = FALSE}
joined_uhf_34_42 = 
  uhf_zip_df |> 
  left_join(y = uhf_34_df, by = join_by("uhf" == "uhf42")) |> 
  rename("uhf34_neighborhood" = "neighborhood.y", 
         "uhf42_neighborhood" = "neighborhood.x", 
         "uhf42" = "uhf")
```

***

## <span style='color: blue;'>Creating One Final Dataframe</span>

Next we needed to merge the neighborhood crosswalk dataframe with the citibike dataframe and each individual variable dataframe. 
