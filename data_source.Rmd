---
title: "<span style='color: blue;'>Data Sources and Overview</span>"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: yeti
css: styles.css
---

<style>
  h1 {
     font-weight: bold;
  }
</style>

  &nbsp;
  
```{r library, include = FALSE}
library(tidyverse)
library(shiny)
library(shinyjs)
shinyjs::useShinyjs()
library(pdftools)
library(pdftables)
```  
  
  ***
  
## <span style='color: blue;'>Data Sources</span>

 &nbsp;

[**CitiBike Data**](https://s3.amazonaws.com/tripdata/index.html): These datasets contain monthly logs of trip, including ride ID, the user's status (subscriber or casual user), trip start/end longitude/latitude coordinates, and the trip start/stop times and duration. 

[**Social Deprivation Index**](https://www.graham-center.org/maps-data-tools/social-deprivation-index.html): The social deprivation index (SDI) is an effort to generate a scoring system of socioeconomic factors using US census data from the American Community Survey. The final SDI is a composite measure of percent living in poverty, percent with less than 12 years of education, percent single-parent households, the percentage living in rented housing units, the percentage living in the overcrowded housing unit, percent of households without a car, and percentage nonemployed adults under 65 years of age. For more details please see the webpage link.

[**Overweight Data**](https://a816-dohbesp.nyc.gov/IndicatorPublic/Subtopic.aspx?theme_code=2,3&subtopic_id=113): This dataset contains publicly available data from NYC regarding the percent of people who are overweight per area.  

[**Air Quality Index Data**](https://data.cityofnewyork.us/Environment/Air-Quality/c3uy-2p5r): These data originated from NYC Open Data, contributed by the Department of Hygiene and Mental Health and include air quality indexes measured across boroughs in NYC.

  &nbsp;

***

## <span style='color: blue;'>Individual Data Imports and Cleaning</span>

  &nbsp;

### *Citibike* ###

   &nbsp;

##### Data Import #####

The citibike data was a collection of csv files that we downloaded and merged. We were specifically interested in 2019 as this was when SDI scores were available.


```{r import citibike}
citibike = 
  tibble(
    files = list.files("./citibike"),
    path = str_c("./citibike/", files)
  ) |>
  mutate(data = map(path, ~read_csv(.x, col_types = cols(
    'end station id' = col_double(),
    'start station id' = col_double()
  )))) |>
  unnest(cols = c(data))
```

```{r show first few rows of uncleaned citibike data}
citibike |>
  head() |>
  knitr::kable()
```

The citibike dataframe includes `r nrow(citibike)` trips and `r ncol(citibike)` variables. The variables in the dataframe are `r names(citibike)`. 

  &nbsp;

##### Data Cleaning #####

The cleaning of the citibike data included the following steps:

 * Recoding gender from numeric to text
 * Filtering trip duration to more than 5 minutes but less than 1 day in order to capture true trips taken
 * Including trips that started in 2018 but ended in 2019 and trips that started in 2019 but ended in 2020, so as to include all trips taken in 2019
 * Creating an age at time of trip variable from birth year
 * Creating a trip duration in minutes (as opposed to seconds) variable
 * Renaming variables
 
```{r tidy citibike}
citibike_df <- citibike |>
  janitor::clean_names() |>
  select(-files, -path) |>
  rename(trip_duration_sec = tripduration,
         start_time = starttime,
         stop_time = stoptime,
         user_type = usertype)|>
  mutate(gender = recode(gender,
                         "0" = "Unknown",
                         "1" = "Male",
                         "2" = "Female"),
        trip_duration_min = trip_duration_sec / 60,
        age = 2019 - birth_year
        ) |>
  filter(
    trip_duration_sec >= 300,
    trip_duration_sec <= 86400,
    as.Date(stop_time) >= as.Date("2019-01-01"),
    as.Date(start_time) <= as.Date("2019-12-31")
  )  |>
  select(trip_duration_sec, trip_duration_min, -start_station_id, -end_station_id, everything())

```

The table below displays the citibike tidied data.

```{r show first few rows of cleaned citibike data}
citibike_df |>
  head() |>
  knitr::kable()
```

The citibike dataframe includes `r nrow(citibike_df)` trips and `r ncol(citibike_df)` variables. The variables in the dataframe are `r names(citibike_df)`. 

  &nbsp;

### *Social Deprivation Index* ###

   &nbsp;

##### Data Import and Cleaning #####

The SDI dataset was a csv. We renamed the zip code variable to be more intuitive. As we were only interested in the entirety of the SDI score, we filtered the data to include zip code and sdi_score only. 

* It is important to note that the higher the SDI score the higher the social deprivation in that area.

```{r sdi import and clean}
SDI_df <- read_csv("data/SDI_data/rgcsdi-2015-2019-zcta.csv") |>
  janitor::clean_names() |>
  select(zcta5_fips, sdi_score)|>
  rename(zip=zcta5_fips)
```

This table shows the first few rows of the sdi dataframe.

```{r sdi clean dataframe}
SDI_df |>
  head() |>
  knitr::kable()
```

 &nbsp;

### *Overweight Data* ###

   &nbsp;

##### Data Import #####

The overweight data was a  csv files that we downloaded and merged. We were specifically interested in 2019 as this was when SDI scores were available.

```{r import overweight data}
overweight = read_csv('./data/SDI_data/nyc_overweight_or_obesity_adults.csv') |>
    janitor::clean_names()
```

 &nbsp;

##### Data Cleaning #####

The cleaning of the overweight data included the following steps:

* Converting percent from a character variable to a numeric variable
* Separating out the high and low confidence intervals of percent
* Renaming time to year
* Filtering data to 2019

```{r tidy overweight}
overweight_df =
 overweight |>
  mutate(number = gsub("\\*", "", number)) |>
  mutate(number = gsub(",", "", number)) |>
  mutate(number = as.numeric(number)) |>
  mutate(
    percent_low = as.numeric(gsub("^.*\\(\\s*", "", gsub("\\s*,.*$", "", percent))),
    percent_high = as.numeric(gsub("^.*\\,\\s*", "", gsub("\\)$", "", percent))),
percent = as.numeric(ifelse(grepl("\\*", percent),
                                gsub("\\*.*$", "", percent),
                                gsub("\\s*\\(.*", "", percent)))) |>
  rename(year = time)|>
  filter (year == 2019)
```

The table below displays the overweight tidied data.

```{r show first few rows of cleaned overweight data}
overweight_df |>
  head() |>
  knitr::kable()
```

The overweight dataframe now includes `r nrow(overweight_df)` rows and `r ncol(overweight_df)` variables. The variables in the dataframe are `r names(overweight_df)`. 

 &nbsp;

### *Air Quality Index* ###


##### Data Import #####

The air quality data was a  csv files. 

```{r air quality}
air <- read_csv("data/air_quality/Air_Quality_20231126.csv") 
```
 &nbsp;

##### Data Cleaning #####

The cleaning of the air quality index data included the following steps:

* Separating out year 
* Filtering data to 2019

```{r air quality tidy}
 air_quality_df =air |>
  janitor::clean_names() |>
  mutate(
    start_date = mdy(start_date),
    year = year(start_date)
  ) |>
  filter(year == "2019")
```

The table below displays the air quality tidied data.

```{r show first few rows of cleaned air quality data}
 air_quality_df |>
  head() |>
  knitr::kable()
```

The air quality dataframe now includes `r nrow( air_quality_df)` rows and `r ncol( air_quality_df)` variables. The variables in the dataframe are `r names( air_quality_df)`.

***

## <span style='color: blue;'>Data Merging by NYC Neighborhood</span>

Our end goal was to merge data by neighborhood. We decided to use United Hospital (UHF) 42 codes which consists of 34 adjoining zip codes with similar characteristics compiled by NYC Gov. We used UFH 34 (instead of other UFH versions) as UFH 34 had the least number of neighborhoods (contains the largest area per neighborhood)

  * For more information about the background behind UHF codes, please see [**here**](https://a816-dohbesp.nyc.gov/IndicatorPublic/data-stories/geographies/). 


 We did the following:

1. First we combined United Hospital (UHF) 42 codes to zip codes, as the overweight dataframe included UHF 42 codes. We found a table that mapped UFH 42 codes to zip codes from a NYC gov  [**pdf**](https://www.nyc.gov/assets/doh/downloads/pdf/ah/zipcodetable.pdf`). We imported this table and tidied it into a dataframe.

```{r import and tidy UFH42 pdf}
# install.packages("pdftables")
# install.packages("pdftools")

uhf_zip = 
  pdf_text("data/geocoding/zipcodetable.pdf")

uhf_zip_df =
  uhf_zip |> 
  str_split("\n") |>
  tibble() |> 
  rename("data" = "str_split(uhf_zip, \"\\n\")") |> 
  unnest("data") |> 	
  filter(data != "") |> 
  filter(!row_number() %in% c(1, 2, 51)) |> 
  filter(!row_number() %in% c(1, 2, 10, 22, 33, 44)) |> 
  mutate(data = str_squish(data)) |> 
  mutate(data = str_remove_all(data, "-")) |> 
  mutate(data = str_squish(data)) |> 
  separate(data, into = c("uhf", "rest"), sep = "(?<=\\d\\s)") |> 
  separate(col = "rest", into = c("neighborhood", "zip"), sep = "(?=\\s\\d)", extra = "merge") |> 
  separate(col = "zip", into = c("zip1", "zip2", "zip3", "zip4", "zip5", "zip6", "zip7", "zip8", "zip9"), sep = ",") |> 
  mutate(across(everything(), ~ str_trim(.x))) |> 
  mutate(across(c("zip1":"zip9"), ~ as.numeric(.x))) |> 
  mutate(uhf = as.numeric(uhf)) |> 
  pivot_longer(c(zip1:zip9), names_to = "zip_name", values_to = "zip") |> 
  filter(!is.na(zip)) |> 
  select(!zip_name)
```

2. We imported and cleaned a dataframe that converted UFH 34 to zip code.

```{r import and tidy UFH 34 pdf}
uhf_34 = 
  pdf_text("data/geocoding/uhf34.pdf")

uhf_34_df =
  uhf_34 |> 
  str_split("\n") |>
  tibble() |> 
  rename("data" = "str_split(uhf_34, \"\\n\")") |> 
  unnest("data") |> 
  filter(data != "") |> 
  mutate(data = str_squish(data)) |> 
  filter(!row_number() %in% c(1, 2, 7, 14, 18, 21, 24, 27, 28, 31, 32, 34, 35, 39, 40, 42, 43, 44, 45, 47, 48, 52, 53, 57, 61:75)) |> 
  mutate(data = str_replace(data, "Kingsbridge - Riverdale", "101 Kingsbridge - Riverdale")) |> 
  mutate(data = str_replace(data, "Northeast Bronx", "102 Northeast Bronx")) |> 
  filter(!row_number() %in% c(1,2)) |> 
  mutate(data = str_remove_all(data, "-"),
         data = str_squish(data), 
         data = str_trim(data)) |>
  mutate(data = str_remove(data, "(\\d)+$")) |>
  mutate(data = str_remove(data, "(\\d)+\\s$")) |>
  separate(data, into = c("uhf34", "neighborhood"), sep = "(?<=\\d\\s)") |> 
  mutate(uhf34 = as.numeric(str_trim(uhf34)),
         neighborhood = str_trim(neighborhood)) |> 
  mutate(uhf2 = uhf34) |> 
  separate_wider_position(uhf2, widths = c("1_uhf" = 3, "2_uhf" = 3, "3_uhf" = 3), too_few = "align_start") |> 
  pivot_longer(c("1_uhf":"3_uhf"), names_to = "uhf_name", values_to = "uhf42") |> 
  select(!uhf_name) |> 
  mutate(uhf34 = as.numeric(uhf34),
         uhf42 = as.numeric(uhf42))
  
```

3. We combined UFH 42 and UFH 34 dataframes to create a neighborhood crosswalk dataframe.

```{r combine neighborhood crosswalk}
joined_uhf_34_42 = 
  uhf_zip_df |> 
  left_join(y = uhf_34_df, by = join_by("uhf" == "uhf42")) |> 
  rename("uhf34_neighborhood" = "neighborhood.y", 
         "uhf42_neighborhood" = "neighborhood.x", 
         "uhf42" = "uhf")
```

***

## <span style='color: blue;'>Creating One Final Dataframe</span>

Next we needed to create one large dataframe that merged the citibike dataframe and each individual variable dataframe by UFH 34 neighborhood. 

We did the following:

1. Join SDI data to UHF/Zip/Neighborhood data
```{r, eval = FALSE}
joined_SDI_zip_neighborhood = 
  SDI_df |> 
  filter(zip %in% pull(joined_uhf_34_42, zip)) |> 
  mutate(zip = as.numeric(zip)) |> 
  left_join(y = joined_uhf_34_42, by = "zip")
```

2. Join overweight data to UHF/Zip/Neighborhood data
```{r, eval = FALSE}
joined_overweight_zip_neighborhood = 
  overweight_df |> 
  left_join(y = joined_uhf_34_42, by = join_by("geo_id" == "uhf34")) |>
  rename(percent_overweight = percent)
```

3. Join overweight and sdi dataframes by uhf42

```{r join_SDI_overweight, eval = FALSE}
sdi_overweight = 
  merge(joined_overweight_zip_neighborhood, joined_SDI_zip_neighborhood,
                   by.x = c("zip", "uhf42", "uhf42_neighborhood"),
                   by.y = c("zip", "uhf42", "uhf42_neighborhood"),
                   all.x = TRUE)  |>
  select("zip","uhf34","uhf34_neighborhood.x","uhf42","uhf42_neighborhood",
         "sdi_score","percent_overweight") |>
  rename(uhf34_neighborhood = uhf34_neighborhood.x) |>
  filter(!is.na(zip)) 

```

4. Join citibike data to neighborhood data

 * First, we need to convert the Citibike location data from latitude/longitude to zip code.  
 * Then crosswalk to the UHF neighborhoods.

```{r citibike_zip, eval = FALSE}
latlong_zip = read_csv('./data/geocoding/citibike_latlong_zip.csv')

citibike_zip =
  citibike_df |>
  left_join(latlong_zip, 
            by = c("start_station_latitude" = "latitude",
                   "start_station_longitude" = "longitude")) |>
  rename("start_zipcode" = "postcode") |>
  left_join(latlong_zip, 
            by = c("end_station_latitude" = "latitude",
                   "end_station_longitude" = "longitude")) |>
  rename("end_zipcode" = "postcode") 

citibike_zip_neighborhoods =
  citibike_zip |>
  left_join(y = (joined_uhf_34_42 |> select(zip, uhf42_neighborhood, uhf34_neighborhood)),
            by = join_by("start_zipcode" == "zip")) |> 
  rename("start_uhf34_neighborhood" = "uhf34_neighborhood",
         "start_uhf42_neighborhood" = "uhf42_neighborhood") |> 
  left_join(y = (joined_uhf_34_42 |> select(zip, uhf42_neighborhood, uhf34_neighborhood)),
            by = join_by("end_zipcode" == "zip")) |> 
  rename("end_uhf34_neighborhood" = "uhf34_neighborhood",
         "end_uhf42_neighborhood" = "uhf42_neighborhood")
```

5. Merge SDI and overweight dataframe onto citibike dataframe

```{r citibike_final, eval = FALSE}

colnames(joined_overweight_zip_neighborhood)
colnames(joined_SDI_zip_neighborhood)

citibike_df =
  citibike_zip_neighborhoods |>
  left_join(y = sdi_overweight,
            by = join_by("start_zipcode" == "zip")) |>
  rename("start_sdi_score" = "sdi_score",
         "start_percent_overweight" = "percent_overweight") 

citibike_df = citibike_df |>
  left_join(y = sdi_overweight,
            by = join_by("end_zipcode" == "zip")) |>
  rename("end_sdi_score" = "sdi_score",
         "end_percent_overweight" = "percent_overweight") 

citibike_df = citibike_df |>
  select(-uhf34.x, -uhf34_neighborhood.x, -uhf42.x, 
         -uhf42_neighborhood.x, -uhf34.x, -uhf34_neighborhood.x, 
         -uhf42.x, - uhf42_neighborhood.x)
```