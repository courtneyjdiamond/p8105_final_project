---
title: "<span style='color: blue;'>Data Sources and Overview</span>"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: yeti
css: styles.css
---

<style>
  h1 {
     font-weight: bold;
  }
</style>

  &nbsp;
  
```{r library, include = FALSE}
library(tidyverse)
library(shiny)
library(shinyjs)
shinyjs::useShinyjs()
library(pdftools)
library(pdftables)

options(warn = -1)
```  
  
  ***
  
## <span style='color: blue;'>Data Sources</span>

 &nbsp;

[**CitiBike Data**](https://s3.amazonaws.com/tripdata/index.html): These datasets contain monthly logs of trip, including ride ID, the user's status (subscriber or casual user), trip start/end longitude/latitude coordinates, and the trip start/stop times and duration. 

[**Social Deprivation Index**](https://www.graham-center.org/maps-data-tools/social-deprivation-index.html): The social deprivation index (SDI) is an effort to generate a scoring system of socioeconomic factors using US census data from the American Community Survey. The final SDI is a composite measure of percent living in poverty, percent with less than 12 years of education, percent single-parent households, the percentage living in rented housing units, the percentage living in the overcrowded housing unit, percent of households without a car, and percentage nonemployed adults under 65 years of age. For more details please see the webpage link.

[**Overweight Data**](https://a816-dohbesp.nyc.gov/IndicatorPublic/Subtopic.aspx?theme_code=2,3&subtopic_id=113): This dataset contains publicly available data from NYC regarding the percent of people who are overweight per area.  

[**Air Quality Index Data**](https://data.cityofnewyork.us/Environment/Air-Quality/c3uy-2p5r): These data originated from NYC Open Data, contributed by the Department of Hygiene and Mental Health and include air quality indexes measured across boroughs in NYC.

  &nbsp;

***

## <span style='color: blue;'>Individual Data Imports and Cleaning</span>

  &nbsp;

### *Citibike* ###

   &nbsp;

##### Data Import #####

The citibike data was a collection of csv files that we downloaded and merged. We were specifically interested in 2019 as this was when SDI scores were available. We downloaded the data files from December 2018 through January 2020.


```{r import citibike}
citibike = 
  tibble(
    files = list.files("./citibike"),
    path = str_c("./citibike/", files)
  ) |>
  mutate(data = map(path, ~read_csv(.x, col_types = cols(
    'end station id' = col_double(),
    'start station id' = col_double()
  )))) |>
  unnest(cols = c(data))
```

```{r show first few rows of uncleaned citibike data}
citibike |>
  head() |>
  knitr::kable()
```

The citibike dataframe includes `r nrow(citibike)` trips and `r ncol(citibike)` variables. The variables in the dataframe are `r names(citibike)`. 

  &nbsp;

##### Data Cleaning #####

The cleaning of the citibike data included the following steps:

 * Recoding gender from numeric to text
 * Filtering trip duration to more than 5 minutes but less than 1 day in order to capture true trips taken
 * Including trips that started in 2018 but ended in 2019 and trips that started in 2019 but ended in 2020, so as to include all trips taken in 2019
 * Creating an age at time of trip variable from birth year
 * Creating a trip duration in minutes (as opposed to seconds) variable
 * Renaming variables
 
```{r tidy citibike}
citibike_df <- citibike |>
  janitor::clean_names() |>
  select(-files, -path, -start_station_id, -end_station_id) |>
  rename(trip_duration_sec = tripduration,
         start_time = starttime,
         stop_time = stoptime,
         user_type = usertype)|>
  mutate(gender = recode(gender,
                         "0" = "Unknown",
                         "1" = "Male",
                         "2" = "Female"),
        trip_duration_min = trip_duration_sec / 60,
        age = 2019 - birth_year
        ) |>
  filter(
    trip_duration_sec >= 300,
    trip_duration_sec <= 86400,
    as.Date(stop_time) >= as.Date("2019-01-01"),
    as.Date(start_time) <= as.Date("2019-12-31")
  )  |>
  select(trip_duration_sec, trip_duration_min, everything())

```

The table below displays the citibike tidied data.

```{r show first few rows of cleaned citibike data}
citibike_df |>
  head() |>
  knitr::kable()
```

The citibike dataframe includes `r nrow(citibike_df)` trips and `r ncol(citibike_df)` variables. The variables in the dataframe are `r names(citibike_df)`. 

  &nbsp;

### *Social Deprivation Index* ###

   &nbsp;

##### Data Import and Cleaning #####

The SDI dataset was a csv. We renamed the zip code variable to be more intuitive. As we were only interested in the entirety of the SDI score, we filtered the data to include zip code and sdi_score only. 

* It is important to note that the higher the SDI score the higher the social deprivation in that area.

```{r sdi import and clean}
SDI_df <- read_csv("data/SDI_data/rgcsdi-2015-2019-zcta.csv") |>
  janitor::clean_names() |>
  select(zcta5_fips, sdi_score)|>
  rename(zip=zcta5_fips)
```

This table shows the first few rows of the sdi dataframe.

```{r sdi clean dataframe}
SDI_df |>
  head() |>
  knitr::kable()
```

 &nbsp;

### *Overweight Data* ###

   &nbsp;

##### Data Import #####

The overweight data was a  csv files that we downloaded and merged. We were specifically interested in 2019 as this was when SDI scores were available.

```{r import overweight data}
overweight = read_csv('./data/SDI_data/nyc_overweight_or_obesity_adults.csv') |>
    janitor::clean_names()
```

 &nbsp;

##### Data Cleaning #####

The cleaning of the overweight data included the following steps:

* Converting percent from a character variable to a numeric variable
* Separating out the high and low confidence intervals of percent
* Renaming time to year
* Filtering data to 2019

```{r tidy overweight}
overweight_df =
 overweight |>
  mutate(number = gsub("\\*", "", number)) |>
  mutate(number = gsub(",", "", number)) |>
  mutate(number = as.numeric(number)) |>
  mutate(
    percent_low = as.numeric(gsub("^.*\\(\\s*", "", gsub("\\s*,.*$", "", percent))),
    percent_high = as.numeric(gsub("^.*\\,\\s*", "", gsub("\\)$", "", percent))),
percent = as.numeric(ifelse(grepl("\\*", percent),
                                gsub("\\*.*$", "", percent),
                                gsub("\\s*\\(.*", "", percent)))) |>
  rename(year = time)|>
  filter (year == 2019)
```

The table below displays the overweight tidied data.

```{r show first few rows of cleaned overweight data}
overweight_df |>
  head() |>
  knitr::kable()
```

The overweight dataframe now includes `r nrow(overweight_df)` rows and `r ncol(overweight_df)` variables. The variables in the dataframe are `r names(overweight_df)`. 

 &nbsp;

### *Air Quality Index* ###


##### Data Import #####

The air quality data was a  csv files. 

```{r air quality}
air <- read_csv("data/air_quality/Air_Quality_20231126.csv") 
```
 &nbsp;

##### Data Cleaning #####

The cleaning of the air quality index data included the following steps:

* Separating out year 
* Filtering data to 2019

```{r air quality tidy}
 air_quality_df =air |>
  janitor::clean_names() |>
  mutate(
    start_date = mdy(start_date),
    year = year(start_date)
  ) |>
  filter(year == "2019")
```

The table below displays the air quality tidied data.

```{r show first few rows of cleaned air quality data}
 air_quality_df |>
  head() |>
  knitr::kable()
```

The air quality dataframe now includes `r nrow( air_quality_df)` rows and `r ncol( air_quality_df)` variables. The variables in the dataframe are `r names( air_quality_df)`.

***

## <span style='color: blue;'>Data Merging by NYC Neighborhood</span>

Our end goal was to merge data by neighborhood. We decided to use United Hospital (UHF) 42 codes which consists of 34 adjoining zip codes with similar characteristics compiled by NYC Gov. We used UFH 34 (instead of other UFH versions) as UFH 34 had the least number of neighborhoods (contains the largest area per neighborhood)

  * For more information about the background behind UHF codes, please see [**here**](https://a816-dohbesp.nyc.gov/IndicatorPublic/data-stories/geographies/). 


 We did the following:

1. First we combined United Hospital (UHF) 42 codes to zip codes, as the overweight dataframe included UHF 42 codes. We found a table that mapped UFH 42 codes to zip codes from a NYC gov  [**pdf**](https://www.nyc.gov/assets/doh/downloads/pdf/ah/zipcodetable.pdf`). We imported this table and tidied it into a dataframe.

```{r import and tidy UFH42 pdf}
# install.packages("pdftables")
# install.packages("pdftools")

uhf_zip = 
  pdf_text("data/geocoding/zipcodetable.pdf")

uhf_zip_df =
  uhf_zip |> 
  str_split("\n") |>
  tibble() |> 
  rename("data" = "str_split(uhf_zip, \"\\n\")") |> 
  unnest("data") |> 	
  filter(data != "") |> 
  filter(!row_number() %in% c(1, 2, 51)) |> 
  filter(!row_number() %in% c(1, 2, 10, 22, 33, 44)) |> 
  mutate(data = str_squish(data)) |> 
  mutate(data = str_remove_all(data, "-")) |> 
  mutate(data = str_squish(data)) |> 
  separate(data, into = c("uhf", "rest"), sep = "(?<=\\d\\s)") |> 
  separate(col = "rest", into = c("neighborhood", "zip"), sep = "(?=\\s\\d)", extra = "merge") |> 
  separate(col = "zip", into = c("zip1", "zip2", "zip3", "zip4", "zip5", "zip6", "zip7", "zip8", "zip9"), sep = ",") |> 
  mutate(across(everything(), ~ str_trim(.x))) |> 
  mutate(across(c("zip1":"zip9"), ~ as.numeric(.x))) |> 
  mutate(uhf = as.numeric(uhf)) |> 
  pivot_longer(c(zip1:zip9), names_to = "zip_name", values_to = "zip") |> 
  filter(!is.na(zip)) |> 
  select(!zip_name)
```

2. We imported and cleaned a dataframe that converted UFH 34 to zip code.

```{r import and tidy UFH 34 pdf}
uhf_34 = 
  pdf_text("data/geocoding/uhf34.pdf")

uhf_34_df =
  uhf_34 |> 
  str_split("\n") |>
  tibble() |> 
  rename("data" = "str_split(uhf_34, \"\\n\")") |> 
  unnest("data") |> 
  filter(data != "") |> 
  mutate(data = str_squish(data)) |> 
  filter(!row_number() %in% c(1, 2, 7, 14, 18, 21, 24, 27, 28, 31, 32, 34, 35, 39, 40, 42, 43, 44, 45, 47, 48, 52, 53, 57, 61:75)) |> 
  mutate(data = str_replace(data, "Kingsbridge - Riverdale", "101 Kingsbridge - Riverdale")) |> 
  mutate(data = str_replace(data, "Northeast Bronx", "102 Northeast Bronx")) |> 
  filter(!row_number() %in% c(1,2)) |> 
  mutate(data = str_remove_all(data, "-"),
         data = str_squish(data), 
         data = str_trim(data)) |>
  mutate(data = str_remove(data, "(\\d)+$")) |>
  mutate(data = str_remove(data, "(\\d)+\\s$")) |>
  separate(data, into = c("uhf34", "neighborhood"), sep = "(?<=\\d\\s)") |> 
  mutate(uhf34 = as.numeric(str_trim(uhf34)),
         neighborhood = str_trim(neighborhood)) |> 
  mutate(uhf2 = uhf34) |> 
  separate_wider_position(uhf2, widths = c("1_uhf" = 3, "2_uhf" = 3, "3_uhf" = 3), too_few = "align_start") |> 
  pivot_longer(c("1_uhf":"3_uhf"), names_to = "uhf_name", values_to = "uhf42") |> 
  select(!uhf_name) |> 
  mutate(uhf34 = as.numeric(uhf34),
         uhf42 = as.numeric(uhf42))
  
```

3. We combined UFH 42 and UFH 34 dataframes to create a neighborhood crosswalk dataframe.

```{r combine neighborhood crosswalk}
joined_uhf_34_42 = 
  uhf_zip_df |> 
  left_join(y = uhf_34_df, by = join_by("uhf" == "uhf42")) |> 
  rename("uhf34_neighborhood" = "neighborhood.y", 
         "uhf42_neighborhood" = "neighborhood.x", 
         "uhf42" = "uhf")
```

***

## <span style='color: blue;'>Joining citibike to UFH 34 Neighborhood Dataframe</span>

The citibike data contained latitude/longitude, but we wanted our final location variable to be UHF 34 neighborhood. To do this, we did the following:


1. First, we need to convert the Citibike location data from latitude/longitude to zip code.This was done using a package [tidygeocoder](https://jessecambon.github.io/tidygeocoder/). Here, we import the latitude/longitude to zipcode crosswalk we generated. The full code with details of how that was created [can be found here](latlong_to_zip.html). 
 

```{r citibike geocode}
latlong_zip = read_csv('./data/geocoding/citibike_latlong_zip.csv')

citibike_zip =
  citibike_df |>
  left_join(latlong_zip, 
            by = c("start_station_latitude" = "latitude",
                   "start_station_longitude" = "longitude")) |>
  rename("start_zipcode" = "postcode") |>
  left_join(latlong_zip, 
            by = c("end_station_latitude" = "latitude",
                   "end_station_longitude" = "longitude")) |>
  rename("end_zipcode" = "postcode") 
```

There are `r citibike_zip |> filter(is.na(start_zipcode) | is.na(end_zipcode)) |> nrow()` entries missing either the start or end zipcode.

2. So next, we manually added the missing zipcodes into the dataframe.
 
```{r manual_zip}
citibike_zip = citibike_zip |>
  mutate(start_zipcode = case_when(start_station_name == "Broadway & W 32 St" ~ 10001,
                               start_station_name == "Cooper Square & Astor Pl" ~ 10003,
                               start_station_name == "William St & Pine St" ~ 10005,
                               start_station_name %in% c("Broadway & W 36 St","Broadway & W 37 St","Broadway & W 38 St","Broadway & W 41 St") ~ 10018,
                               start_station_name == "W 52 St & 6 Ave" ~ 100019,
                               start_station_name %in% c("Broadway & W 41 St","W 43 St & 6 Ave") ~ 10036,
                               start_station_name == "South St & Gouverneur Ln" ~ 10043,
                               start_station_name == "E 47 St & 1 Ave" ~ 10075,
                               start_station_name == "Roebling St & N 4 St" ~ 11211,
                               TRUE ~ start_zipcode)) |>
  mutate(end_zipcode = case_when(end_station_name == "Broadway & W 32 St" ~ 10001,
                               end_station_name == "Cooper Square & Astor Pl" ~ 10003,
                               end_station_name == "William St & Pine St" ~ 10005,
                               end_station_name %in% c("Broadway & W 36 St","Broadway & W 37 St","Broadway & W 38 St","Broadway & W 41 St") ~ 10018,
                               end_station_name == "W 52 St & 6 Ave" ~ 100019,
                               end_station_name %in% c("Broadway & W 41 St","W 43 St & 6 Ave") ~ 10036,
                               end_station_name == "South St & Gouverneur Ln" ~ 10043,
                               end_station_name == "E 47 St & 1 Ave" ~ 10075,
                               end_station_name == "Roebling St & N 4 St" ~ 11211,
                               TRUE ~ end_zipcode))
```
  
3. Then we merged the citibike dataframe with the neighborhood dataframe by zip code.

```{r citibike_neighborhoods}
citibike_zip_neighborhoods =
  citibike_zip |>
  left_join(y = (joined_uhf_34_42 |> select(zip, uhf42_neighborhood, uhf34_neighborhood)),
            by = join_by("start_zipcode" == "zip")) |> 
  rename("start_uhf34_neighborhood" = "uhf34_neighborhood",
         "start_uhf42_neighborhood" = "uhf42_neighborhood") |> 
  left_join(y = (joined_uhf_34_42 |> select(zip, uhf42_neighborhood, uhf34_neighborhood)),
            by = join_by("end_zipcode" == "zip")) |> 
  rename("end_uhf34_neighborhood" = "uhf34_neighborhood",
         "end_uhf42_neighborhood" = "uhf42_neighborhood")
```

We performed a check to see if there were missing UHF 34 codes.

```{r missing_uhf}
missing_startuhf = citibike_zip_neighborhoods |>
  filter(is.na(start_uhf34_neighborhood)) |>
  group_by(start_zipcode) |>
  summarize(n = n()) |>
  rename(zipcode = start_zipcode)
  
missing_enduhf = citibike_zip_neighborhoods |>
  filter(is.na(end_uhf34_neighborhood)) |>
  group_by(end_zipcode) |>
  summarize(n = n()) |>
  rename(zipcode = end_zipcode)

citibike_zip_neighborhoods = citibike_zip_neighborhoods |>
  filter(is.na(end_uhf34_neighborhood) | is.na(start_uhf34_neighborhood))
```

We noticed that there were `r missing_startuhf |> pull(n) |> sum()` entries whose start zipcode do not have an associated uhf34, and  `r missing_enduhf |> pull(n) |> sum()` entries whose end zipcode do not have an associated uhf34. This corresponded to `r length(missing_startuhf |> pull(zipcode) |> na.omit())` start zip codes and `r length(missing_enduhf |> pull(zipcode) |> na.omit())` end zip codes. Manual validation showed these zipcodes are business zipcodes, where a zipcode refers to a small domain of a business. While these can be pinpointed roughly to a UHF34 neighborhood manually, it is cleaner to simply omit them, as we still retain a substantial dataset size. The resulting dataset has `r citibike_zip_neighborhoods |> nrow()` entries. 

***

## <span style='color: blue;'>Finally, Merge Citibike, Social Deprivation Index, Overweight, and Air Quality Index Data  </span>

Next we needed to create one large dataframe that merged the citibike dataframe and each individual variable dataframe by UFH 34 neighborhood. 

We did the following:

1. Join SDI data to neighborhood dataframe
```{r SDI to neighborhood}
joined_SDI_zip_neighborhood = 
  SDI_df |> 
  filter(zip %in% pull(joined_uhf_34_42, zip)) |> 
  mutate(zip = as.numeric(zip)) |> 
  left_join(y = joined_uhf_34_42, by = "zip")
```

2. Join overweight data to neighborhood dataframe
```{r overweight to neighborhood}
joined_overweight_zip_neighborhood = 
  overweight_df |> 
  left_join(y = joined_uhf_34_42, by = join_by("geo_id" == "uhf34")) |>
  rename(percent_overweight = percent)
```

3. Join overweight and sdi dataframes by uhf42

```{r join_SDI_overweight}
sdi_overweight = 
  merge(joined_overweight_zip_neighborhood, joined_SDI_zip_neighborhood,
                   by.x = c("zip", "uhf42", "uhf42_neighborhood"),
                   by.y = c("zip", "uhf42", "uhf42_neighborhood"),
                   all.x = TRUE)  |>
  select("zip","uhf34","uhf34_neighborhood.x","uhf42","uhf42_neighborhood",
         "sdi_score","percent_overweight") |>
  rename(uhf34_neighborhood = uhf34_neighborhood.x) |>
  filter(!is.na(zip)) 

```

4. Join air quality to neighborhood datafame
```{r air_quality_uhf}
air_quality_df = 
  left_join(air_quality_df, uhf_34_df, by = c("geo_join_id" = "uhf34"))

air_quality_df =
  air_quality_df |>
select(data_value, neighborhood ) |>
 distinct()
```

5. Join SDI/Overweight dataframe with citibike dataframe
```{r SDI_Overweight_join}
citibike_df =
  citibike_zip_neighborhoods |>
  left_join(y = sdi_overweight,
            by = join_by("start_zipcode" == "zip")) |>
  rename("start_sdi_score" = "sdi_score",
         "start_percent_overweight" = "percent_overweight") 

remove(citibike_zip_neighborhoods)

citibike_df = citibike_df |>
  left_join(y = sdi_overweight,
            by = join_by("end_zipcode" == "zip")) |>
  rename("end_sdi_score" = "sdi_score",
         "end_percent_overweight" = "percent_overweight") 
```

6. Join air quality dataframe with citibike dataframe

```{r AQ_join, eval = FALSE}
citibike_df =
  citibike_df |>
  left_join(y = air_quality_df,
            by = join_by("start_uhf34_neighborhood" == "neighborhood")) |>
  rename("start_aq" = "data_value")

citibike_df = citibike_df |>
  left_join(y = air_quality_df,
            by = join_by("end_uhf34_neighborhood" == "neighborhood")) |>
  rename("end_aq" = "data_value")

```

7. Perform final Tidy of Citibike Data 

```{r final_citibike_tidy, eval = FALSE}
citibike_df = citibike_df |>
  select(bikeid, user_type, gender, age,
         start_time, stop_time, 
         start_station_id, start_station_name,
         start_zipcode, start_uhf34_neighborhood,
         end_station_id, end_station_name,
         end_zipcode, end_uhf34_neighborhood,
         start_sdi_score, start_percent_overweight, start_aq,
         end_sdi_score, end_percent_overweight, end_aq)
```

The table below shows the first few rows of the final version of the citibike dataframe. 
```{r final dataframe table, eval = FALSE}
head(citibike_df) |>
  knitr::kable()
```

The citibike dataframe includes r nrow(citibike) trips and r ncol(citibike) variables. The variables in the dataframe are r names(citibike).
